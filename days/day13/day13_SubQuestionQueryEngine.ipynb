{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1879aab9-c20d-4fc2-a0f4-dc83cb018f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a587586-07dd-45ea-bb49-f4f47eb97a0b",
   "metadata": {},
   "source": [
    "# 1. import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c246d8d-7115-4665-9173-70ea54752195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "from llama_index.core.workflow import (\n",
    "    step,\n",
    "    Context,\n",
    "    Workflow,\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    ")\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "from llama_index.tools.tavily_research.base import TavilyToolSpec\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fe8ee8-8b78-4789-8efd-84d4b26fac15",
   "metadata": {},
   "source": [
    "# 2. input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c5ef53-f1ec-4e22-82ec-1ae7e05baee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qset = {\n",
    "  \"id\": \"113-1-1-med-surg\",\n",
    "  \"year\": \"113\",\n",
    "  \"time\": \"1\",\n",
    "  \"qid\": \"1\",\n",
    "  \"discipline\": \"內外科護理學\",\n",
    "  \"ans\": \"C\",\n",
    "  \"question\": \"有關多發性硬化症之診斷檢查，下列何者錯誤？\",\n",
    "  \"options\": {\n",
    "   \"A\": \"腦脊髓液分析可發現IgG抗體上升\",\n",
    "   \"B\": \"視覺誘發電位可觀察到受損的神經在傳導過程出現延遲和中斷\",\n",
    "   \"C\": \"超音波檢查可發現中樞神經系統髓鞘脫失\",\n",
    "   \"D\": \"核磁共振影像可用來確認多發性硬化症之斑塊\"\n",
    "  },\n",
    "  \"discipline_slug\": \"med-surg\"\n",
    "}\n",
    "exam_question = f\"題幹: {qset['question']}\\n選項: \\nA: {qset['options']['A']}; B: {qset['options']['B']}; C: {qset['options']['C']}; D: {qset['options']['D']}.\"\n",
    "print(exam_question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5baedf2-00b4-47bf-adcf-3480d4164a2a",
   "metadata": {},
   "source": [
    "# 3. prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd60f8-df1f-4168-a484-193d6ae13245",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB_QUESTION_PROMPT = PromptTemplate(\"\"\"你是一個考題拆解助手。請根據以下單選題，產生一系列子問題。規則如下：\n",
    "\n",
    "1. 子問題需為單一問句，避免複合句。\n",
    "2. 每個子問題必須包含完整上下文，不可依賴原始題目才能理解。\n",
    "3. 子問題的集合在合併答案後，應能完整回答此單選題。\n",
    "4. 回應必須是**純 JSON 格式**，不得包含任何額外文字或 Markdown。\n",
    "\n",
    "### 範例輸出：\n",
    "{{\n",
    "  \"sub_questions\": [\n",
    "    \"舊金山的人口是多少？\",\n",
    "    \"舊金山的年度預算是多少？\",\n",
    "    \"舊金山的 GDP 是多少？\"\n",
    "  ]\n",
    "}}\n",
    "\n",
    "以下是單選題：\n",
    "{exam_question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(SUB_QUESTION_PROMPT.format(exam_question=exam_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2805835-78d0-4fa6-9cca-df7c5752077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReAct Agent prompt\n",
    "AGENT_PROMPT = PromptTemplate(\"\"\"你被設計來協助處理各種任務，包含提供查詢到的資料用以回答問題、提供摘要以及其他類型的分析。\n",
    "並且為了確保資訊的更新，請盡可能的在有相關查詢內容的情況下才回答。\n",
    "\n",
    "## 工具\n",
    "\n",
    "你可以使用各式各樣的工具。你需要自行決定使用這些工具的順序來完成任務。\n",
    "這可能需要將任務拆解成子任務，並使用不同的工具來完成每個子任務。\n",
    "\n",
    "你能使用以下工具：\n",
    "{tool_desc}\n",
    "\n",
    "\n",
    "## 輸出格式\n",
    "\n",
    "請用與問題相同的語言回答，並使用以下格式：\n",
    "\n",
    "```\n",
    "\n",
    "Thought: 我需要使用一個工具來協助回答問題。\n",
    "Action: 工具名稱 (必須是 {tool_names} 之一，如果要用工具的話)\n",
    "Action Input: 工具的輸入，必須是 JSON 格式，代表 kwargs (例如 {{\"input\": \"hello world\", \"num_beams\": 5}})\n",
    "\n",
    "```\n",
    "\n",
    "請務必**總是以 Thought 開始**。\n",
    "\n",
    "絕對不要用 Markdown 的程式碼區塊把整個回應包起來。但如果需要，你可以在回應中使用程式碼區塊。\n",
    "\n",
    "請務必使用合法的 JSON 格式作為 Action Input。\n",
    "如果你包含了 \"Action:\" 這一行，那麼你也必須包含 \"Action Input:\" 這一行，即使該工具不需要任何參數，此時你也必須寫 \"Action Input: {{}}\"。\n",
    "\n",
    "如果使用這個格式，工具會回傳以下格式的結果：\n",
    "\n",
    "```\n",
    "\n",
    "Observation: 工具的回應\n",
    "\n",
    "```\n",
    "\n",
    "你需要重複上述格式，直到你有足夠的資訊來回答問題而不再需要使用工具。  \n",
    "這時你必須用以下兩種格式之一來回答：\n",
    "\n",
    "```\n",
    "\n",
    "Thought: 我已經可以不用更多工具就回答問題。\n",
    "Answer: [在這裡寫下你的答案]\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "Thought: 我無法使用提供的工具回答問題。\n",
    "Answer: [在這裡寫下你的答案]\n",
    "\n",
    "```\n",
    "\n",
    "## 當前對話\n",
    "\n",
    "以下是目前的對話內容，由使用者與助手交錯的訊息組成：\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ede00-4160-4883-97d8-e5b79e6b3092",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINE_ANSWER_PROMPT = PromptTemplate(\"\"\"你是一個考題作答助手。以下是一題單選題，已經被拆解成數個子問題，並且每個子問題都已有答案。\n",
    "請將所有子問題的答案整合，產生一個完整且連貫的最終解答，以回答原始單選題。\n",
    "\n",
    "以下是單選題：\n",
    "{exam_question}\n",
    "\n",
    "子問題與答案：\n",
    "{sub_qa}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53989a59-1850-43ef-9ad4-da5468d171ec",
   "metadata": {},
   "source": [
    "# 4. Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0d31f-76b1-4f40-ae7f-0b2549583168",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryEvent(Event):\n",
    "    question: str\n",
    "\n",
    "\n",
    "class AnswerEvent(Event):\n",
    "    question: str\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef91977-603c-43b9-958b-b6b4dc52741c",
   "metadata": {},
   "source": [
    "# 5. run_agent_with_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc9cb1-f883-4ba5-9b1d-0e23451ae536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import AgentInput, AgentOutput, ToolCall, ToolCallResult\n",
    "\n",
    "async def run_agent_with_stream(agent, query):\n",
    "    handler = agent.run(query)\n",
    "    results = []\n",
    "\n",
    "    async for ev in handler.stream_events(expose_internal=False):\n",
    "        name = ev.__class__.__name__\n",
    "        print(f\"-----stream event: {name}\")\n",
    "        results.append((name, ev))\n",
    "\n",
    "        if isinstance(ev, AgentInput):\n",
    "            print(f\"len of chat message: {len(ev.input)}\")\n",
    "        elif isinstance(ev, AgentOutput):\n",
    "            print(ev.response.blocks[0].text)\n",
    "        elif isinstance(ev, ToolCall):\n",
    "            print(f\"{ev.tool_name}: {ev.tool_kwargs}\")\n",
    "        elif isinstance(ev, ToolCallResult):\n",
    "            num_rv = len(ev.tool_output.blocks)\n",
    "            print(f\"num_result: {num_rv}\")\n",
    "\n",
    "    # 最終 response\n",
    "    response = await handler\n",
    "    return results, response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4209d251-e03f-4872-930f-362bf66fee31",
   "metadata": {},
   "source": [
    "# 6. workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aae010-02c2-47fa-9606-792472480b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubQuestionQueryEngine(Workflow):\n",
    "    @step\n",
    "    async def query(self, ctx: Context, ev: StartEvent) -> QueryEvent:\n",
    "        # initial\n",
    "        llm = OpenAI(model=\"gpt-5-mini\", temperature=0, json_mode=True)\n",
    "        # subquestions gen\n",
    "        print('sub question gen...')\n",
    "        response = llm.complete(SUB_QUESTION_PROMPT.format(exam_question=ev.question))\n",
    "        print('sub question gen complete')\n",
    "        sub_questions = json.loads(response.text)['sub_questions']\n",
    "        # get num_question\n",
    "        num_question = len(sub_questions)\n",
    "        await ctx.store.set(\"num_question\", len(sub_questions))\n",
    "        await ctx.store.set(\"exam_question\", exam_question)\n",
    "        for idx, q in enumerate(sub_questions):\n",
    "            print(f\"send Q{idx+1}: {q}\")\n",
    "            ctx.send_event(QueryEvent(question=q))\n",
    "        return None\n",
    "\n",
    "    @step\n",
    "    async def sub_question(self, ctx: Context, ev: QueryEvent) -> AnswerEvent:\n",
    "        # initial\n",
    "        tavily_tool = TavilyToolSpec(\n",
    "            api_key=TAVILY_API_KEY,\n",
    "        )\n",
    "        tavily_tool_list = tavily_tool.to_tool_list()\n",
    "        llm = OpenAI(model=\"gpt-5-mini\", temperature=0, is_streaming=False)  # streaming False for non-verified organisations\n",
    "        agent = ReActAgent(tools=tavily_tool_list, llm=llm, streaming=False, verbose=False)\n",
    "        agent.update_prompts({\"react_header\": AGENT_PROMPT})\n",
    "        # call\n",
    "        results, response = await run_agent_with_stream(agent, ev.question)\n",
    "        answer = response.response.blocks[0].text\n",
    "        return AnswerEvent(question=ev.question, answer=answer)\n",
    "\n",
    "    @step\n",
    "    async def combine_answers(\n",
    "        self, ctx: Context, ev: AnswerEvent\n",
    "    ) -> StopEvent | None:\n",
    "        num_question = await ctx.store.get(\"num_question\")\n",
    "        exam_question = await ctx.store.get(\"exam_question\")\n",
    "        # wait until we receive all events\n",
    "        result = ctx.collect_events(ev, [AnswerEvent] * num_question)\n",
    "        if result is None:\n",
    "            print('combine_answers waite None')\n",
    "            return None\n",
    "        # combine sub_question and sub_answer\n",
    "        sub_qa = \"\\n\\n\".join([\n",
    "            f\"Question: {qa.question}: \\n Answer: {qa.answer}\"\n",
    "            for qa in result\n",
    "        ])\n",
    "        \n",
    "        llm = OpenAI(model=\"gpt-5-mini\", temperature=0, is_streaming=False)\n",
    "        response = llm.complete(COMBINE_ANSWER_PROMPT.format(sub_qa=sub_qa, exam_question=exam_question))\n",
    "        return StopEvent(result=response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6104a084-936b-4212-9941-f640ec26a2ff",
   "metadata": {},
   "source": [
    "# 7. workflow visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3744d0ea-5b89-4e61-ac39-554f02a660fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_all_possible_flows(\n",
    "    SubQuestionQueryEngine, filename=\"day13_sub_question_query_engine.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7e889-db79-45ca-8ef7-8fdee12873aa",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4df214-9e55-4bba-aa5f-ccfd5c668bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = SubQuestionQueryEngine(timeout=600, verbose=False)\n",
    "\n",
    "handler = w.run(start_event=StartEvent(question=exam_question))\n",
    "response = await handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a27b0e-2cef-4e28-b204-b863f7055ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42fd4b4-646f-483a-85e7-ecdf593ce6ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag30",
   "language": "python",
   "name": "rag30"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

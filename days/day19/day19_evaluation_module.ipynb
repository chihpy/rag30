{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52289812-d09c-4fd3-9916-c51a26c38efc",
   "metadata": {},
   "source": [
    "# reference: \n",
    "- [SemanticSimilarityEvaluator](https://developers.llamaindex.ai/python/examples/evaluation/semantic_similarity_eval)\n",
    "- [CorrectnessEvaluator](https://developers.llamaindex.ai/python/examples/evaluation/correctness_eval)\n",
    "- [FaithfulnessEvaluator](http://localhost:8888/notebooks/Faithfulness_Evaluator.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a86492a-1df2-4cdd-9e22-2fae085ccdf8",
   "metadata": {},
   "source": [
    "## get test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f9d5b9-60ca-4906-a192-dc0c1af24109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': '1.常見針灸配穴法中，所指的「四關穴」，為下列何穴位之組合？\\n\\xa0\\nA.上星、日月\\nB.合谷、太衝\\nC.內關、外關\\nD.上關、下關',\n",
       " 'reference_answer': {'qid': '1',\n",
       "  'stem': '常見針灸配穴法中,所指的「四關穴」,為下列何穴位之組合?',\n",
       "  'A': '上星、日月',\n",
       "  'B': '合谷、太衝',\n",
       "  'C': '內關、外關',\n",
       "  'D': '上關、下關'},\n",
       " 'json_gemma_response': {'qid': 1,\n",
       "  'stem': '常見針灸配穴法中，所指的「四關穴」，為下列何穴位之組合？',\n",
       "  'A': '上星、日月',\n",
       "  'B': '合谷、太衝',\n",
       "  'C': '內關、外關',\n",
       "  'D': '上關、下關'},\n",
       " 'llama_en_response': {'A': '主蒙，曜月',\n",
       "  'B': '合座，夡里',\n",
       "  'C': '到递，割递',\n",
       "  'D': '主递，一递',\n",
       "  'qid': 1,\n",
       "  'stem': '台九気动组化。\\n\\nA.主蒙，曜月\\nB.合座，夡里\\nC.到递，割递\\nD.主递，一递'}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def json_load(file_path):\n",
    "    #print(\"load data from: \" + file_path)\n",
    "    with open(file_path, 'r') as f:\n",
    "        data_dict = json.load(f)\n",
    "    return data_dict\n",
    "\n",
    "test_data_file_path = os.path.join('data', 'source', 'example_test_data.json')\n",
    "test_data = json_load(test_data_file_path)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c5bfe-573d-41de-a482-d414832a8913",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3c9f468-d12e-49e3-91db-9e1d5c0a4b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.evaluation import SemanticSimilarityEvaluator\n",
    "from llama_index.core.evaluation import CorrectnessEvaluator\n",
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "llm = OpenAI(model=\"gpt-5-mini\", temperature=0, is_streaming=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc66c62-57ae-4ea7-bbd4-f02decd4a34e",
   "metadata": {},
   "source": [
    "# SemanticSimilarityEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "608c6e76-70fb-49b7-9f95-fb1ba74f409a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "SemanticSimilarityEvaluator(\n",
       "    embed_model: Optional[llama_index.core.base.embeddings.base.BaseEmbedding] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    similarity_fn: Optional[Callable[..., float]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    similarity_mode: Optional[llama_index.core.base.embeddings.base.SimilarityMode] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    similarity_threshold: float = \u001b[32m0.8\u001b[39m,\n",
       ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "Embedding similarity evaluator.\n",
       "\n",
       "Evaluate the quality of a question answering system by\n",
       "comparing the similarity between embeddings of the generated answer\n",
       "and the reference answer.\n",
       "\n",
       "Inspired by this paper:\n",
       "- Semantic Answer Similarity for Evaluating Question Answering Models\n",
       "    https://arxiv.org/pdf/2108.06130.pdf\n",
       "\n",
       "Args:\n",
       "    similarity_threshold (float): Embedding similarity threshold for \"passing\".\n",
       "        Defaults to 0.8.\n",
       "\u001b[31mFile:\u001b[39m           ~/miniconda3/envs/rag30/lib/python3.12/site-packages/llama_index/core/evaluation/semantic_similarity.py\n",
       "\u001b[31mType:\u001b[39m           ABCMeta\n",
       "\u001b[31mSubclasses:\u001b[39m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SemanticSimilarityEvaluator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a0d87a6-8e65-470e-a5c0-1a9af31b22f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.9967628031093531\n",
      "Passing:  True\n"
     ]
    }
   ],
   "source": [
    "semantic_evaluator = SemanticSimilarityEvaluator(similarity_threshold=0.8)\n",
    "gd_result = await semantic_evaluator.aevaluate(\n",
    "    response=str(test_data['json_gemma_response']),\n",
    "    reference=str(test_data['reference_answer']),\n",
    ")\n",
    "print(\"Score: \", gd_result.score)\n",
    "print(\"Passing: \", gd_result.passing)  # use similarity threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0332573c-d18a-44e3-8eb3-fd82f5233f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8674033244211179\n",
      "Passing:  True\n"
     ]
    }
   ],
   "source": [
    "bd_result = await semantic_evaluator.aevaluate(\n",
    "    response=str(test_data['llama_en_response']),\n",
    "    reference=str(test_data['reference_answer']),\n",
    ")\n",
    "print(\"Score: \", bd_result.score)\n",
    "print(\"Passing: \", bd_result.passing)  # use similarity threshold "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95eb642-0b62-43f6-b746-c6ae5b89ace8",
   "metadata": {},
   "source": [
    "# CorrectnessEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39a00339-c820-4a88-8a87-33c8fa638590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "CorrectnessEvaluator(\n",
       "    llm: Optional[llama_index.core.llms.llm.LLM] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    eval_template: Union[llama_index.core.prompts.base.BasePromptTemplate, str, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    score_threshold: float = \u001b[32m4.0\u001b[39m,\n",
       "    parser_function: Callable[[str], Tuple[Optional[float], Optional[str]]] = <function default_parser at \u001b[32m0x75669cef9620\u001b[39m>,\n",
       ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "Correctness evaluator.\n",
       "\n",
       "Evaluates the correctness of a question answering system.\n",
       "This evaluator depends on `reference` answer to be provided, in addition to the\n",
       "query string and response string.\n",
       "\n",
       "It outputs a score between 1 and 5, where 1 is the worst and 5 is the best,\n",
       "along with a reasoning for the score.\n",
       "Passing is defined as a score greater than or equal to the given threshold.\n",
       "\n",
       "Args:\n",
       "    eval_template (Optional[Union[BasePromptTemplate, str]]):\n",
       "        Template for the evaluation prompt.\n",
       "    score_threshold (float): Numerical threshold for passing the evaluation,\n",
       "        defaults to 4.0.\n",
       "\u001b[31mFile:\u001b[39m           ~/miniconda3/envs/rag30/lib/python3.12/site-packages/llama_index/core/evaluation/correctness.py\n",
       "\u001b[31mType:\u001b[39m           ABCMeta\n",
       "\u001b[31mSubclasses:\u001b[39m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CorrectnessEvaluator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "019943f7-96b8-4dfe-b7e8-bf3a928f2e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_evaluator = CorrectnessEvaluator(llm = llm, score_threshold=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d596a9b6-4989-4bfd-9cb9-34b40185f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_en_llama = PromptTemplate(\n",
    "    \"Extract a multiple-choice question (MCQ) from the following text.\\n\"\n",
    "    \"If the original text does not provide an answer, \"\n",
    "    \"omit the answer field entirely and do not attempt to guess it: {text}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90c9b199-b209-4171-8281-d8f1dc4fabf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedback: The generated MCQ matches the reference exactly in stem and options (only minor non-substantive difference in qid formatting as an integer vs string). It correctly omits an answer field as required.\n",
      "score: 5.0\n",
      "passing: True\n"
     ]
    }
   ],
   "source": [
    "query = prompt_en_llama.format(text=test_data['context'])\n",
    "result = correct_evaluator.evaluate(\n",
    "    query=query,\n",
    "    response=str(test_data['json_gemma_response']),\n",
    "    reference=str(test_data['reference_answer']),\n",
    ")\n",
    "print(f\"feedback: {result.feedback}\")\n",
    "print(f\"score: {result.score}\")\n",
    "print(f\"passing: {result.passing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d498f8ad-ff12-47ba-a4aa-6dc03c73906e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedback: The generated answer is incorrect and largely garbled: the stem text is nonsensical and the options do not match the original question or the reference answer. It fails to extract the correct MCQ fields and provides wrong characters/words, so it is not a valid extraction.\n",
      "score: 1.0\n",
      "passing: False\n"
     ]
    }
   ],
   "source": [
    "query = prompt_en_llama.format(text=test_data['context'])\n",
    "result = correct_evaluator.evaluate(\n",
    "    query=query,\n",
    "    response=str(test_data['llama_en_response']),\n",
    "    reference=str(test_data['reference_answer']),\n",
    ")\n",
    "print(f\"feedback: {result.feedback}\")\n",
    "print(f\"score: {result.score}\")\n",
    "print(f\"passing: {result.passing}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5afd495-ac7d-4acb-a216-d1a0fd8a6b7a",
   "metadata": {},
   "source": [
    "DEFAULT_SYSTEM_TEMPLATE = \"\"\"\n",
    "You are an expert evaluation system for a question answering chatbot.\n",
    "\n",
    "You are given the following information:\n",
    "- a user query, and\n",
    "- a generated answer\n",
    "\n",
    "You may also be given a reference answer to use for reference in your evaluation.\n",
    "\n",
    "Your job is to judge the relevance and correctness of the generated answer.\n",
    "Output a single score that represents a holistic evaluation.\n",
    "You must return your response in a line with only the score.\n",
    "Do not return answers in any other format.\n",
    "On a separate line provide your reasoning for the score as well.\n",
    "\n",
    "Follow these guidelines for scoring:\n",
    "- Your score has to be between 1 and 5, where 1 is the worst and 5 is the best.\n",
    "- If the generated answer is not relevant to the user query, \\\n",
    "you should give a score of 1.\n",
    "- If the generated answer is relevant but contains mistakes, \\\n",
    "you should give a score between 2 and 3.\n",
    "- If the generated answer is relevant and fully correct, \\\n",
    "you should give a score between 4 and 5.\n",
    "\n",
    "Example Response:\n",
    "4.0\n",
    "The generated answer has the exact same metrics as the reference answer, \\\n",
    "    but it is not as concise.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "DEFAULT_USER_TEMPLATE = \"\"\"\n",
    "## User Query\n",
    "{query}\n",
    "\n",
    "## Reference Answer\n",
    "{reference_answer}\n",
    "\n",
    "## Generated Answer\n",
    "{generated_answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3cfbed-dc77-4b78-91ae-44073c1058c5",
   "metadata": {},
   "source": [
    "# FaithfulnessEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df75a0c4-9a98-4e63-9c97-c679c2c656af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "FaithfulnessEvaluator(\n",
       "    llm: \u001b[33m'Optional[LLM]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    raise_error: \u001b[33m'bool'\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    eval_template: \u001b[33m'Optional[Union[str, BasePromptTemplate]]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    refine_template: \u001b[33m'Optional[Union[str, BasePromptTemplate]]'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       ") -> \u001b[33m'None'\u001b[39m\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "Faithfulness evaluator.\n",
       "\n",
       "Evaluates whether a response is faithful to the contexts\n",
       "(i.e. whether the response is supported by the contexts or hallucinated.)\n",
       "\n",
       "This evaluator only considers the response string and the list of context strings.\n",
       "\n",
       "Args:\n",
       "    raise_error(bool): Whether to raise an error when the response is invalid.\n",
       "        Defaults to False.\n",
       "    eval_template(Optional[Union[str, BasePromptTemplate]]):\n",
       "        The template to use for evaluation.\n",
       "    refine_template(Optional[Union[str, BasePromptTemplate]]):\n",
       "        The template to use for refining the evaluation.\n",
       "\u001b[31mInit docstring:\u001b[39m Init params.\n",
       "\u001b[31mFile:\u001b[39m           ~/miniconda3/envs/rag30/lib/python3.12/site-packages/llama_index/core/evaluation/faithfulness.py\n",
       "\u001b[31mType:\u001b[39m           ABCMeta\n",
       "\u001b[31mSubclasses:\u001b[39m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FaithfulnessEvaluator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8c33276-e2dd-4b25-bb3d-5ea2ab4b9a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = FaithfulnessEvaluator(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55c61bb2-bfe0-4f5a-bdd2-fb3bcc7805d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m\n",
       "evaluator.evaluate(\n",
       "    query: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    response: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    contexts: Optional[Sequence[str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    **kwargs: Any,\n",
       ") -> llama_index.core.evaluation.base.EvaluationResult\n",
       "\u001b[31mDocstring:\u001b[39m\n",
       "Run evaluation with query string, retrieved contexts,\n",
       "and generated response string.\n",
       "\n",
       "Subclasses can override this method to provide custom evaluation logic and\n",
       "take in additional arguments.\n",
       "\u001b[31mFile:\u001b[39m      ~/miniconda3/envs/rag30/lib/python3.12/site-packages/llama_index/core/evaluation/base.py\n",
       "\u001b[31mType:\u001b[39m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluator.evaluate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e11ff1c6-3e2d-4135-b29e-682bfaea6ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedback: YES, passing: True, score: 1.0\n"
     ]
    }
   ],
   "source": [
    "eval_result = evaluator.evaluate(contexts=[test_data['context']], response=str(test_data['json_gemma_response']))\n",
    "print(f\"feedback: {eval_result.feedback}, passing: {eval_result.passing}, score: {eval_result.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c7a0dda-b885-4d48-baca-fcc45380a54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedback: NO, passing: False, score: 0.0\n"
     ]
    }
   ],
   "source": [
    "fake_response = test_data['json_gemma_response']\n",
    "fake_response['ans'] = 'C'\n",
    "eval_result = evaluator.evaluate(contexts=[test_data['context']], response=str(fake_response))\n",
    "print(f\"feedback: {eval_result.feedback}, passing: {eval_result.passing}, score: {eval_result.score}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c16312f5-181f-49d0-a0a9-edece072a219",
   "metadata": {},
   "source": [
    "DEFAULT_EVAL_TEMPLATE = PromptTemplate(\n",
    "    \"Please tell if a given piece of information \"\n",
    "    \"is supported by the context.\\n\"\n",
    "    \"You need to answer with either YES or NO.\\n\"\n",
    "    \"Answer YES if any of the context supports the information, even \"\n",
    "    \"if most of the context is unrelated. \"\n",
    "    \"Some examples are provided below. \\n\\n\"\n",
    "    \"Information: Apple pie is generally double-crusted.\\n\"\n",
    "    \"Context: An apple pie is a fruit pie in which the principal filling \"\n",
    "    \"ingredient is apples. \\n\"\n",
    "    \"Apple pie is often served with whipped cream, ice cream \"\n",
    "    \"('apple pie à la mode'), custard or cheddar cheese.\\n\"\n",
    "    \"It is generally double-crusted, with pastry both above \"\n",
    "    \"and below the filling; the upper crust may be solid or \"\n",
    "    \"latticed (woven of crosswise strips).\\n\"\n",
    "    \"Answer: YES\\n\"\n",
    "    \"Information: Apple pies tastes bad.\\n\"\n",
    "    \"Context: An apple pie is a fruit pie in which the principal filling \"\n",
    "    \"ingredient is apples. \\n\"\n",
    "    \"Apple pie is often served with whipped cream, ice cream \"\n",
    "    \"('apple pie à la mode'), custard or cheddar cheese.\\n\"\n",
    "    \"It is generally double-crusted, with pastry both above \"\n",
    "    \"and below the filling; the upper crust may be solid or \"\n",
    "    \"latticed (woven of crosswise strips).\\n\"\n",
    "    \"Answer: NO\\n\"\n",
    "    \"Information: {query_str}\\n\"\n",
    "    \"Context: {context_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "DEFAULT_REFINE_TEMPLATE = PromptTemplate(\n",
    "    \"We want to understand if the following information is present \"\n",
    "    \"in the context information: {query_str}\\n\"\n",
    "    \"We have provided an existing YES/NO answer: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing answer \"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"If the existing answer was already YES, still answer YES. \"\n",
    "    \"If the information is present in the new context, answer YES. \"\n",
    "    \"Otherwise answer NO.\\n\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag30",
   "language": "python",
   "name": "rag30"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

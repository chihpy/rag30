{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9cb3abee-9f2d-47a3-a138-ce738645fddf",
   "metadata": {},
   "source": [
    "reference: https://github.com/run-llama/llama_index/blob/main/docs/examples/workflow/corrective_rag_pack.ipynb\n",
    "paper: https://arxiv.org/pdf/2401.15884"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d474e5-0701-4ddc-818f-b44cc02073b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ece35-2ab1-4646-ba39-f57423707c74",
   "metadata": {},
   "source": [
    "# Designing the Workflow"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d158dae-abee-4934-815a-49f5ae3e5278",
   "metadata": {},
   "source": [
    "- Ingesting of data\n",
    "    - Loads the data into an index\n",
    "    - setting up Tavily\n",
    "    - taking in a start event and returning a stop event.\n",
    "\n",
    "- Retrieval:\n",
    "    - Retrives the most relevant nodes based on the query\n",
    "\n",
    "- Relevance extraction:\n",
    "    - Extracts the nodes which the LLM determined to be relevant.\n",
    "\n",
    "- Query transformation and Tavily search:\n",
    "    - if a node is irrelevant, then uses an LLM to transform the query to tailor towards a websearch.\n",
    "    - Use Tavily to search the web for a relevant answer based on the query\n",
    "\n",
    "- Response generation:\n",
    "    - Builds a summary index given the text from the relevant nodes and the Tavily search and uses this index to get a result given the original query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2e1e5-2c60-4374-bee1-90080d453703",
   "metadata": {},
   "source": [
    "# Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c26bbba8-9d6e-477c-9a41-18d11afcef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "class PrepEvent(Event):\n",
    "    \"\"\"Prep event (prepares for retrieval).\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class RetrieveEvent(Event):\n",
    "    \"\"\"Retrieve event (gets retrieved nodes).\"\"\"\n",
    "\n",
    "    retrieved_nodes: list[NodeWithScore]\n",
    "\n",
    "\n",
    "class RelevanceEvalEvent(Event):\n",
    "    \"\"\"Relevance evaluation event (gets results of relevance evaluation).\"\"\"\n",
    "\n",
    "    relevant_results: list[str]\n",
    "\n",
    "\n",
    "class TextExtractEvent(Event):\n",
    "    \"\"\"Text extract event. Extracts relevant text and concatenates.\"\"\"\n",
    "\n",
    "    relevant_text: str\n",
    "\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    \"\"\"Query event. Queries given relevant text and search text.\"\"\"\n",
    "\n",
    "    relevant_text: str\n",
    "    search_text: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e8acb7a-8133-4b2b-b120-4d31b96e9bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    ")\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    Document,\n",
    "    PromptTemplate,\n",
    "    SummaryIndex,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.tools.tavily_research.base import TavilyToolSpec\n",
    "from llama_index.core.base.base_retriever import BaseRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e197ea7-4682-445e-bda3-797cdd7f8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_RELEVANCY_PROMPT_TEMPLATE = PromptTemplate(\n",
    "    template=\"\"\"As a grader, your task is to evaluate the relevance of a document retrieved in response to a user's question.\n",
    "\n",
    "    Retrieved Document:\n",
    "    -------------------\n",
    "    {context_str}\n",
    "\n",
    "    User Question:\n",
    "    --------------\n",
    "    {query_str}\n",
    "\n",
    "    Evaluation Criteria:\n",
    "    - Consider whether the document contains keywords or topics related to the user's question.\n",
    "    - The evaluation should not be overly stringent; the primary objective is to identify and filter out clearly irrelevant retrievals.\n",
    "\n",
    "    Decision:\n",
    "    - Assign a binary score to indicate the document's relevance.\n",
    "    - Use 'yes' if the document is relevant to the question, or 'no' if it is not.\n",
    "\n",
    "    Please provide your binary score ('yes' or 'no') below to indicate the document's relevance to the user question.\"\"\"\n",
    ")\n",
    "\n",
    "DEFAULT_TRANSFORM_QUERY_TEMPLATE = PromptTemplate(\n",
    "    template=\"\"\"Your task is to refine a query to ensure it is highly effective for retrieving relevant search results. \\n\n",
    "    Analyze the given input to grasp the core semantic intent or meaning. \\n\n",
    "    Original Query:\n",
    "    \\n ------- \\n\n",
    "    {query_str}\n",
    "    \\n ------- \\n\n",
    "    Your goal is to rephrase or enhance this query to improve its search performance. Ensure the revised query is concise and directly aligned with the intended search objective. \\n\n",
    "    Respond with the optimized query only:\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab71cb-1f38-489d-9725-e9505e3a3eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectiveRAGWorkflow(Workflow):\n",
    "    @step\n",
    "    async def ingest(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n",
    "        \"\"\"document_dir, index_dir\"\"\"\n",
    "        document_dir = ev.get(\"document_dir\")\n",
    "        index_dir = ev.get(\"index_dir\")\n",
    "        if not index_dir:\n",
    "            # 如果沒有 index_dir 這條路就會直接走不通\n",
    "            # 所以 query 的時候預設是不用給 index_dir 的\n",
    "            return None\n",
    "        if not os.path.isdir(index_dir):\n",
    "            print(f'create index_dir: {index_dir}')\n",
    "            os.makedirs(index_dir)\n",
    "        if document_dir:\n",
    "\n",
    "            documents = SimpleDirectoryReader(document_dir).load_data()\n",
    "            d = 1536\n",
    "            faiss_index = faiss.IndexFlatL2(d)\n",
    "            vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "            storage_context = StorageContext.from_defaults(\n",
    "                vector_store=vector_store\n",
    "            )\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents=documents,\n",
    "                embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
    "                storage_context=storage_context,\n",
    "            )\n",
    "            index.storage_context.persist(persist_dir=index_dir)\n",
    "            print(\"Index built and persisted to:\", index_dir)\n",
    "        else:\n",
    "            print(f\"load index from: {index_dir}\")\n",
    "            # load index from disk\n",
    "            vector_store = FaissVectorStore.from_persist_dir(index_dir)\n",
    "            storage_context = StorageContext.from_defaults(\n",
    "                vector_store=vector_store, persist_dir=index_dir\n",
    "            )\n",
    "            index = load_index_from_storage(storage_context=storage_context)\n",
    "        return StopEvent(result=index)\n",
    "\n",
    "    @step\n",
    "    async def prepare_for_retrieval(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> PrepEvent | None:\n",
    "        \"\"\"Prepare for retrieval.\"\"\"\n",
    "\n",
    "        query_str: str | None = ev.get(\"query_str\")\n",
    "        retriever_kwargs: dict | None = ev.get(\"retriever_kwargs\", {})\n",
    "\n",
    "        if query_str is None:\n",
    "            return None\n",
    "\n",
    "#        tavily_ai_apikey: str | None = ev.get(\"tavily_ai_apikey\")\n",
    "        index = ev.get(\"index\")\n",
    "\n",
    "        llm = OpenAI(model=\"gpt5-mini\")\n",
    "\n",
    "        await ctx.store.set(\"llm\", llm)\n",
    "        await ctx.store.set(\"index\", index)\n",
    "        await ctx.store.set(\n",
    "            \"tavily_tool\", TavilyToolSpec(api_key=tavily_ai_apikey)\n",
    "        )\n",
    "\n",
    "        await ctx.store.set(\"query_str\", query_str)\n",
    "        await ctx.store.set(\"retriever_kwargs\", retriever_kwargs)\n",
    "\n",
    "        return PrepEvent()\n",
    "\n",
    "    @step\n",
    "    async def retrieve(\n",
    "        self, ctx: Context, ev: PrepEvent\n",
    "    ) -> RetrieveEvent | None:\n",
    "        \"\"\"Retrieve the relevant nodes for the query.\"\"\"\n",
    "        query_str = await ctx.store.get(\"query_str\")\n",
    "        retriever_kwargs = await ctx.store.get(\"retriever_kwargs\")\n",
    "\n",
    "        if query_str is None:\n",
    "            return None\n",
    "\n",
    "        index = await ctx.store.get(\"index\", default=None)\n",
    "        tavily_tool = await ctx.store.get(\"tavily_tool\", default=None)\n",
    "        if not (index or tavily_tool):\n",
    "            raise ValueError(\n",
    "                \"Index and tavily tool must be constructed. Run with 'documents' and 'tavily_ai_apikey' params first.\"\n",
    "            )\n",
    "\n",
    "        retriever: BaseRetriever = index.as_retriever(**retriever_kwargs)\n",
    "        result = retriever.retrieve(query_str)\n",
    "        await ctx.store.set(\"retrieved_nodes\", result)\n",
    "        await ctx.store.set(\"query_str\", query_str)\n",
    "        return RetrieveEvent(retrieved_nodes=result)\n",
    "\n",
    "    @step\n",
    "    async def eval_relevance(\n",
    "        self, ctx: Context, ev: RetrieveEvent\n",
    "    ) -> RelevanceEvalEvent:\n",
    "        \"\"\"Evaluate relevancy of retrieved documents with the query.\"\"\"\n",
    "        retrieved_nodes = ev.retrieved_nodes\n",
    "        query_str = await ctx.store.get(\"query_str\")\n",
    "\n",
    "        relevancy_results = []\n",
    "        for node in retrieved_nodes:\n",
    "            llm = await ctx.store.get(\"llm\")\n",
    "            resp = await llm.acomplete(\n",
    "                DEFAULT_RELEVANCY_PROMPT_TEMPLATE.format(\n",
    "                    context_str=node.text, query_str=query_str\n",
    "                )\n",
    "            )\n",
    "            relevancy_results.append(resp.text.lower().strip())\n",
    "\n",
    "        await ctx.store.set(\"relevancy_results\", relevancy_results)\n",
    "        return RelevanceEvalEvent(relevant_results=relevancy_results)\n",
    "\n",
    "    @step\n",
    "    async def extract_relevant_texts(\n",
    "        self, ctx: Context, ev: RelevanceEvalEvent\n",
    "    ) -> TextExtractEvent:\n",
    "        \"\"\"Extract relevant texts from retrieved documents.\"\"\"\n",
    "        retrieved_nodes = await ctx.store.get(\"retrieved_nodes\")\n",
    "        relevancy_results = ev.relevant_results\n",
    "\n",
    "        relevant_texts = [\n",
    "            retrieved_nodes[i].text\n",
    "            for i, result in enumerate(relevancy_results)\n",
    "            if result == \"yes\"\n",
    "        ]\n",
    "\n",
    "        result = \"\\n\".join(relevant_texts)\n",
    "        return TextExtractEvent(relevant_text=result)\n",
    "\n",
    "    @step\n",
    "    async def transform_query(\n",
    "        self, ctx: Context, ev: TextExtractEvent\n",
    "    ) -> QueryEvent:\n",
    "        \"\"\"Search the transformed query with Tavily API.\"\"\"\n",
    "        relevant_text = ev.relevant_text\n",
    "        relevancy_results = await ctx.store.get(\"relevancy_results\")\n",
    "        query_str = await ctx.store.get(\"query_str\")\n",
    "\n",
    "        # If any document is found irrelevant, transform the query string for better search results.\n",
    "        if \"no\" in relevancy_results:\n",
    "            llm = await ctx.store.get(\"llm\")\n",
    "            resp = await llm.acomplete(\n",
    "                DEFAULT_TRANSFORM_QUERY_TEMPLATE.format(query_str=query_str)\n",
    "            )\n",
    "            transformed_query_str = resp.text\n",
    "            # Conduct a search with the transformed query string and collect the results.\n",
    "            tavily_tool = await ctx.store.get(\"tavily_tool\")\n",
    "            search_results = tavily_tool.search(\n",
    "                transformed_query_str, max_results=5\n",
    "            )\n",
    "            search_text = \"\\n\".join([result.text for result in search_results])\n",
    "        else:\n",
    "            search_text = \"\"\n",
    "\n",
    "        return QueryEvent(relevant_text=relevant_text, search_text=search_text)\n",
    "\n",
    "    @step\n",
    "    async def query_result(self, ctx: Context, ev: QueryEvent) -> StopEvent:\n",
    "        \"\"\"Get result with relevant text.\"\"\"\n",
    "        relevant_text = ev.relevant_text\n",
    "        search_text = ev.search_text\n",
    "        query_str = await ctx.store.get(\"query_str\")\n",
    "\n",
    "        documents = [Document(text=relevant_text + \"\\n\" + search_text)]\n",
    "        index = SummaryIndex.from_documents(documents)\n",
    "        query_engine = index.as_query_engine()\n",
    "        result = query_engine.query(query_str)\n",
    "        return StopEvent(result=result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag30",
   "language": "python",
   "name": "rag30"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

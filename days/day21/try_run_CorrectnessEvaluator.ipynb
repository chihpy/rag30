{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ad66e9-dc2e-43ed-877f-c9afaf195186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import json_load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba658f62-6f24-4c33-a07f-9d21c13c6fd7",
   "metadata": {},
   "source": [
    "# get pred data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "630a2f11-767d-4983-93f1-0ddce112684e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2_llama_zh_evaluation_dataset.json', '4_gemma_evaluation_dataset.json', '5_json_gemma_evaluation_dataset.json', '3_llama_chat_evaluation_dataset.json', '1_llama_en_evaluation_dataset.json']\n",
      "load data from: data/source/evaluation_dataset/2_llama_zh_evaluation_dataset.json\n",
      "reference_answer\n",
      "{'qid': '1', 'stem': '常見針灸配穴法中,所指的「四關穴」,為下列何穴位之組合?', 'A': '上星、日月', 'B': '合谷、太衝', 'C': '內關、外關', 'D': '上關、下關'}\n",
      "reference_context\n",
      "['1.常見針灸配穴法中，所指的「四關穴」，為下列何穴位之組合？\\n\\xa0\\nA.上星、日月\\nB.合谷、太衝\\nC.內關、外關\\nD.上關、下關']\n",
      "response\n",
      "{'A': '三村台终飯加度。', 'B': '吃条台终飯加度。', 'C': '六南分。三村台终飯加度。', 'D': '一条台终飯加度。', 'ans': None, 'qid': 1, 'stem': '台九気终飯加度泩。吃的“六南分。，是一条有分题不别的宽器。'}\n"
     ]
    }
   ],
   "source": [
    "SOURCE_DIR = os.path.join('data', 'source', 'evaluation_dataset')\n",
    "print(os.listdir(SOURCE_DIR))\n",
    "llama_zh_dataset_file_path = os.path.join(SOURCE_DIR, '2_llama_zh_evaluation_dataset.json')\n",
    "#\n",
    "dataset = json_load(llama_zh_dataset_file_path)\n",
    "\n",
    "example = dataset[0]\n",
    "\n",
    "print('reference_answer')\n",
    "print(example['reference_answer'])\n",
    "print('reference_context')\n",
    "print(example['reference_context'])\n",
    "print('response')\n",
    "print(example['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8774901d-9973-4262-8950-d6068ed1613f",
   "metadata": {},
   "source": [
    "# steup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "670a256b-0122-4d6a-ae7b-0791b0d774ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.evaluation import CorrectnessEvaluator\n",
    "\n",
    "llm = OpenAI(model=\"gpt-5-mini\", temperature=0, is_streaming=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9441a-b7d3-454f-b05e-e09271feb58f",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "336c2c5e-605d-44c4-9675-18f99d8ef942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "correct_evaluator = CorrectnessEvaluator(llm = llm, score_threshold=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5faa03b9-3817-4abe-a5be-2f24a0998aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ChatPromptTemplate\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "system_prompt = (\n",
    "    \"你是一個專業的問答聊天機器人評估系統。\\n\"\n",
    "    \"你會被提供以下資訊：\\n\"\n",
    "    \"- 使用者的問題 (user query)\\n\"\n",
    "    \"- 系統生成的答案 (generated answer)\\n\"\n",
    "    \"同時，你也會被提供一個參考答案 (reference answer)，作為評估的參考依據。\\n\"\n",
    "    \"你的工作是判斷生成答案的「相關性」與「正確性」。\\n\"\n",
    "    \"請輸出一個單一分數，代表整體的評估結果。\\n\"\n",
    "    \"你必須只在第一行輸出分數，不要使用任何其他格式。\\n\"\n",
    "    \"接著，在第二行解釋你為什麼會給這個分數。\\n\"\n",
    "    \"評分準則如下：\\n\"\n",
    "    \"- 分數需介於 1 到 5 之間，1 代表最差，5 代表最好。\\n\"\n",
    "    \"- 如果生成的答案與使用者的問題無關，請給分數 1。\\n\"\n",
    "    \"- 如果生成的答案有關聯，但包含錯誤，請給 2 至 3 分。\\n\"\n",
    "    \"- 如果生成的答案相關且完全正確，請給 4 至 5 分。\\n\"\n",
    "    \"範例回覆：\\n\"\n",
    "    \"4.0\\n\"\n",
    "    \"生成的答案與參考答案在指標上完全相同，但不如參考答案簡潔。\"\n",
    ")\n",
    "\n",
    "user_prompt = (\n",
    "    \"## 使用者問題\\n\"\n",
    "    \"{query}\\n\"\n",
    "    \"## 參考答案\\n\"\n",
    "    \"{reference_answer}\\n\"\n",
    "    \"## 系統生成的答案\\n\"\n",
    "    \"{generated_answer}\"\n",
    ")\n",
    "\n",
    "\n",
    "chat_template = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(\n",
    "            role=MessageRole.SYSTEM,\n",
    "            content=system_prompt),\n",
    "        ChatMessage(\n",
    "            role=MessageRole.USER, \n",
    "            content=user_prompt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1cd16a4-2b4a-43e9-a70d-0cdebf98dcbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mType:\u001b[39m        CorrectnessEvaluator\n",
       "\u001b[31mString form:\u001b[39m <llama_index.core.evaluation.correctness.CorrectnessEvaluator object at 0x7f03a53b17f0>\n",
       "\u001b[31mFile:\u001b[39m        ~/miniconda3/envs/rag30/lib/python3.12/site-packages/llama_index/core/evaluation/correctness.py\n",
       "\u001b[31mDocstring:\u001b[39m  \n",
       "Correctness evaluator.\n",
       "\n",
       "Evaluates the correctness of a question answering system.\n",
       "This evaluator depends on `reference` answer to be provided, in addition to the\n",
       "query string and response string.\n",
       "\n",
       "It outputs a score between 1 and 5, where 1 is the worst and 5 is the best,\n",
       "along with a reasoning for the score.\n",
       "Passing is defined as a score greater than or equal to the given threshold.\n",
       "\n",
       "Args:\n",
       "    eval_template (Optional[Union[BasePromptTemplate, str]]):\n",
       "        Template for the evaluation prompt.\n",
       "    score_threshold (float): Numerical threshold for passing the evaluation,\n",
       "        defaults to 4.0."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "correct_evaluator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03d6e01c-cdb1-4736-937f-01e7fb8c4bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt init\n",
    "correct_evaluator = CorrectnessEvaluator(llm = llm, score_threshold=4.0, eval_template=chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2ab42b6-ee30-4999-b504-df1e075bf12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_template': ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['query', 'reference_answer', 'generated_answer'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='\\n你是一個專業的問答聊天機器人評估系統。\\n\\n你會被提供以下資訊：\\n- 使用者的問題 (user query)\\n- 系統生成的答案 (generated answer)\\n\\n同時，你也可能會被提供一個參考答案 (reference answer)，作為評估的參考依據。\\n\\n你的工作是判斷生成答案的「相關性」與「正確性」。\\n\\n請輸出一個單一分數，代表整體的評估結果。\\n你必須只在第一行輸出分數，不要使用任何其他格式。\\n接著，在第二行解釋你為什麼會給這個分數。\\n\\n評分準則如下：\\n- 分數需介於 1 到 5 之間，1 代表最差，5 代表最好。\\n- 如果生成的答案與使用者的問題無關，請給分數 1。\\n- 如果生成的答案有關聯，但包含錯誤，請給 2 至 3 分。\\n- 如果生成的答案相關且完全正確，請給 4 至 5 分。\\n\\n範例回覆：\\n4.0\\n生成的答案與參考答案在指標上完全相同，但不如參考答案簡潔。\\n')]), ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='## 使用者問題\\n{query}\\n\\n## 參考答案\\n{reference_answer}\\n\\n## 系統生成的答案\\n{generated_answer}\\n')])])}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_evaluator.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde1cd47-ab11-4960-a143-5f1f0138f4c3",
   "metadata": {},
   "source": [
    "# prompt_zh_llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd44ce60-366a-469a-a7df-edc367171884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "prompt_zh_llama = PromptTemplate(\n",
    "    \"從以下文字中擷取一題選擇題 (MCQ)。如果原始文字沒有提供答案，則完全省略答案欄位，且不要嘗試推測答案\\n\\n-----\\n{text}\\n-----\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe32d59d-8d3d-452b-bfd1-4168e01e14e8",
   "metadata": {},
   "source": [
    "# get query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2b8ff839-9151-4b58-ad99-10d59eab3eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "從以下文字中擷取一題選擇題 (MCQ)。如果原始文字沒有提供答案，則完全省略答案欄位，且不要嘗試推測答案\n",
      "\n",
      "-----\n",
      "1.常見針灸配穴法中，所指的「四關穴」，為下列何穴位之組合？\n",
      " \n",
      "A.上星、日月\n",
      "B.合谷、太衝\n",
      "C.內關、外關\n",
      "D.上關、下關\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = prompt_zh_llama.format(text=example['reference_context'][0])\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d554613b-d118-41ec-91fc-a30e4f09e674",
   "metadata": {},
   "source": [
    "# invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "13abb180-8c05-4a65-a271-c363e4386537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedback: 生成答案與題目完全不相干且內容為亂碼/無意義文字，未正確擷取原始題幹與選項（選項與題目不符且被破壞），雖包含 qid 與 ans: None，但未依要求呈現正確的選項欄位與省略答案欄位，因此判定為最差。\n",
      "score: 1.0\n",
      "passing: False\n"
     ]
    }
   ],
   "source": [
    "result = correct_evaluator.evaluate(\n",
    "    query=query,\n",
    "    response=str(example['response']),\n",
    "    reference=str(example['reference_answer']),\n",
    ")\n",
    "print(f\"feedback: {result.feedback}\")\n",
    "print(f\"score: {result.score}\")\n",
    "print(f\"passing: {result.passing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d8fc9b7-c9d7-4e04-bf19-48c2133f6fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b77932d6-da74-46db-ae85-161cbbdeeb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bool"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result.passing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2900e9-d261-4f6d-a3aa-86688d7f22e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag30",
   "language": "python",
   "name": "rag30"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

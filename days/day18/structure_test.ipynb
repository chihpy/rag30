{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4aa58bc4-2e3f-4676-949d-9f1afb828c81",
   "metadata": {},
   "source": [
    "這邊主要想要測試的是:\n",
    "- default prompt 究竟有損無損"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d29fd58-7f67-4fc4-867a-308629fa3c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "#MODEL_NAME = 'llama'  # gemma, mini\n",
    "MODEL_NAME = 'llama'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43044029-a1af-4b2b-8642-3e1570bd8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58906602-de4f-48d8-8af8-e67b642cd6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': '單選題結構，包含題號(qid)、題幹(stem)、以及 A、B、C、D 四個選項',\n",
       " 'properties': {'qid': {'description': '題號',\n",
       "   'title': 'Qid',\n",
       "   'type': 'integer'},\n",
       "  'stem': {'description': '題幹', 'title': 'Stem', 'type': 'string'},\n",
       "  'A': {'description': '本題的A選項', 'title': 'A', 'type': 'string'},\n",
       "  'B': {'description': '本題的B選項', 'title': 'B', 'type': 'string'},\n",
       "  'C': {'description': '本題的C選項', 'title': 'C', 'type': 'string'},\n",
       "  'D': {'description': '本題的D選項', 'title': 'D', 'type': 'string'},\n",
       "  'ans': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'description': '答案',\n",
       "   'title': 'Ans'}},\n",
       " 'required': ['qid', 'stem', 'A', 'B', 'C', 'D'],\n",
       " 'title': 'MCQ',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import MCQ\n",
    "MCQ.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a68ba176-c992-4753-9c6b-857e9795c1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# name: MCQ\n",
      "# description: 單選題結構，包含題號(qid)、題幹(stem)、以及 A、B、C、D 四個選項\n"
     ]
    }
   ],
   "source": [
    "from utils import get_mcq_tool_list\n",
    "\n",
    "mcq_tool_list = get_mcq_tool_list()\n",
    "mcq_tool = mcq_tool_list[0]\n",
    "print(f\"# name: {mcq_tool.metadata.name}\\n# description: {mcq_tool.metadata.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579147a4-a032-4cd9-9489-06b33a607bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use ollama model: llama3.1:latest\n",
      "use ollama model: gemma3:12b\n"
     ]
    }
   ],
   "source": [
    "from utils import get_llm\n",
    "\n",
    "llama = get_llm('llama')\n",
    "gemma = get_llm('gemma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5786f-661f-43de-926a-ec9952ac56fc",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be2ac699-c0d0-44d9-a829-71cfeb181d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from utils import json_load\n",
    "\n",
    "file_path = os.path.join('data/source/structured_output_dataset.json')\n",
    "data = json_load(file_path)\n",
    "data = data['examples']\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c23e03-aff1-4ea0-b3b1-c1a7de9abe2a",
   "metadata": {},
   "source": [
    "# 1. Structured LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a44e3c1-2fd4-432f-a424-b32b2ddb8ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://developers.llamaindex.ai/python/framework/understanding/extraction/structured_llms/\n",
    "# code: https://github.com/run-llama/llama_index/blob/8469a034226d20b70a667dc7faf013770716709f/llama-index-core/llama_index/core/llms/structured_llm.py#L32\n",
    "# note:\n",
    "## - 這個看起來是沒有加任何 prompt，就是 Pydantic 給了 就要他 predict 了\n",
    "## - 底層還是去 call structured prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77ae7796-5663-4b94-8f27-79cd4c6cde49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.常見針灸配穴法中，所指的「四關穴」，為下列何穴位之組合？\\n\\xa0\\nA.上星、日月\\nB.合谷、太衝\\nC.內關、外關\\nD.上關、下關'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = data[0]['reference_context'][0]\n",
    "data[0]['reference_context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abe0f1a3-30fb-4413-b258-44d772022713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': 1,\n",
       " 'stem': '答案是 C',\n",
       " 'A': '上星、日月',\n",
       " 'B': '合谷、太衝',\n",
       " 'C': '內關、外關',\n",
       " 'D': '上關、下關',\n",
       " 'ans': None}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sllama = llama.as_structured_llm(MCQ)\n",
    "query = data[0]['reference_context'][0]\n",
    "response = sllama.complete(query)\n",
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3100666-7600-4125-82cf-cdf04e26dc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': 1,\n",
       " 'stem': '1.常見針灸配穴法中，所指的「四關穴」，為下列何穴位之組合？',\n",
       " 'A': '上星、日月',\n",
       " 'B': '合谷、太衝',\n",
       " 'C': '內關、外關',\n",
       " 'D': '上關、下關',\n",
       " 'ans': 'C'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgemma = gemma.as_structured_llm(MCQ)\n",
    "response = sgemma.complete(query)\n",
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92427dee-89d2-45e9-b682-1de6e871e303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='{\"qid\":1,\"stem\":\"1.常見針灸配穴法中，所指的「四關穴」，為下列何穴位之組合？\",\"A\":\"上星、日月\",\"B\":\"合谷、太衝\",\"C\":\"內關、外關\",\"D\":\"上關、下關\",\"ans\":\"C\"}', additional_kwargs={}, raw=MCQ(qid=1, stem='1.常見針灸配穴法中，所指的「四關穴」，為下列何穴位之組合？', A='上星、日月', B='合谷、太衝', C='內關、外關', D='上關、下關', ans='C'), logprobs=None, delta=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b3460b-385c-4470-b3a5-7d1a2af19048",
   "metadata": {},
   "source": [
    "# 2. structured_predict en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "148ebdb7-ae0a-43d2-a18d-dd8d05e24753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://developers.llamaindex.ai/python/framework/understanding/extraction/structured_prediction/\n",
    "# code: https://github.com/run-llama/llama_index/blob/8469a034226d20b70a667dc7faf013770716709f/llama-index-core/llama_index/core/llms/llm.py#L307"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79130f92-69e7-41a0-b311-3619cbd29e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': 1,\n",
       " 'stem': '',\n",
       " 'A': '上星、日月',\n",
       " 'B': '合谷、太衝',\n",
       " 'C': '內關、外關',\n",
       " 'D': '上關、下關',\n",
       " 'ans': None}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    #\"Extract an MCQ from the following text. If you cannot find an answer, use the default value None and the date as the invoice ID: {text}\"\n",
    "    \"Extract a multiple-choice question (MCQ) from the following text. If the original text does not provide an answer, omit the answer field entirely and do not attempt to guess it: {text}\"\n",
    ")\n",
    "\n",
    "response = llama.structured_predict(\n",
    "    MCQ, prompt, text=query\n",
    ")\n",
    "\n",
    "json.loads(response.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28624fe9-17fd-4a9f-b1d5-c14c68586b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': 1,\n",
       " 'stem': 'In common acupuncture point combinations, what combination of acupoints is referred to as “Si Guan (Four Passes)”?',\n",
       " 'A': 'Shang Xing, Ri Yue',\n",
       " 'B': 'He Gu, Tai Chong',\n",
       " 'C': 'Nei Guan, Wai Guan',\n",
       " 'D': 'Shang Guan, Xia Guan',\n",
       " 'ans': None}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = gemma.structured_predict(\n",
    "    MCQ, prompt, text=query\n",
    ")\n",
    "\n",
    "json.loads(response.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04970ff-9da3-467f-a4b0-8a868d6d78f2",
   "metadata": {},
   "source": [
    "# 3. Structured Prediction zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7088413-b02a-414d-99a7-a6537c4838b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': 1,\n",
       " 'stem': '',\n",
       " 'A': '上星、日月',\n",
       " 'B': '合谷、太衝',\n",
       " 'C': '內關、外關',\n",
       " 'D': '上關、下關',\n",
       " 'ans': None}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    #\"Extract an MCQ from the following text. If you cannot find an answer, use the default value None and the date as the invoice ID: {text}\"\n",
    "    #\"Extract a multiple-choice question (MCQ) from the following text. If the original text does not provide an answer, omit the answer field entirely and do not attempt to guess it: {text}\"\n",
    "    \"從以下文字中擷取一題選擇題 (MCQ)。如果原始文字沒有提供答案，則完全省略答案欄位，且不要嘗試推測答案：{text}\"\n",
    ")\n",
    "\n",
    "response = llama.structured_predict(\n",
    "    MCQ, prompt, text=query\n",
    ")\n",
    "\n",
    "json.loads(response.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "918c9d6e-58d2-45a3-8443-a58f6187deaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': 1,\n",
       " 'stem': '常見針灸配穴法中，所指的「四關穴」，為下列何穴位之組合？',\n",
       " 'A': '上星、日月',\n",
       " 'B': '合谷、太衝',\n",
       " 'C': '內關、外關',\n",
       " 'D': '上關、下關',\n",
       " 'ans': None}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = gemma.structured_predict(\n",
    "    MCQ, prompt, text=query\n",
    ")\n",
    "\n",
    "json.loads(response.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2aece9-339f-46e9-bf1c-407c10198e03",
   "metadata": {},
   "source": [
    "# 4. chat_with_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31e69a2f-6994-4031-982a-20a8e31b9ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m\n",
       "llm.chat_with_tools(\n",
       "    tools: Sequence[ForwardRef(\u001b[33m'BaseTool'\u001b[39m)],\n",
       "    user_msg: Union[str, llama_index.core.base.llms.types.ChatMessage, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    chat_history: Optional[List[llama_index.core.base.llms.types.ChatMessage]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    verbose: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    allow_parallel_tool_calls: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    tool_required: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    **kwargs: Any,\n",
       ") -> llama_index.core.base.llms.types.ChatResponse\n",
       "\u001b[31mDocstring:\u001b[39m Chat with function calling.\n",
       "\u001b[31mFile:\u001b[39m      ~/miniconda3/envs/rag30/lib/python3.12/site-packages/llama_index/core/llms/function_calling.py\n",
       "\u001b[31mType:\u001b[39m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm.chat_with_tools?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff8e8a11-0a58-4040-9afa-26fe579e1a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'A': '代村机,天月', 'B': '合度,天台雨', 'C': '南通,割通', 'D': '代通,三通', 'qid': 1, 'stem': '台九気终飯騻合前。\\n\\nA. 代村机,天月\\nB. 合度,天台雨\\nC. 南通,割通\\nD. 代通,三通'}]\n"
     ]
    }
   ],
   "source": [
    "message = prompt.format_messages(text=query)[0]\n",
    "resp = llama.chat_with_tools(\n",
    "    [mcq_tool],\n",
    "    user_msg=message,\n",
    "    # chat_history\n",
    "    allow_parallel_tool_calls=True,\n",
    "    tool_required=True\n",
    ")\n",
    "\n",
    "tool_calls = llm.get_tool_calls_from_response(\n",
    "    resp, error_on_no_tool_call=False\n",
    ")\n",
    "outputs = []\n",
    "for tool_call in tool_calls:\n",
    "    outputs.append(tool_call.model_dump()['tool_kwargs'])\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c261da8-8d2f-4430-888f-087a08a8e1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': '代村机,天月',\n",
       " 'B': '合度,天台雨',\n",
       " 'C': '南通,割通',\n",
       " 'D': '代通,三通',\n",
       " 'qid': 1,\n",
       " 'stem': '台九気终飯騻合前。\\n\\nA. 代村机,天月\\nB. 合度,天台雨\\nC. 南通,割通\\nD. 代通,三通'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls[0].model_dump()['tool_kwargs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7accbd-4aa4-4118-920a-4dadba90dbb4",
   "metadata": {},
   "source": [
    "# 5. Direct prompting without json mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e324a6ad-e68d-48e2-8d06-93e3a3c8178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這是 MCQ 的 JSON schema:\n",
      "{'description': '單選題結構，包含題號(qid)、題幹(stem)、以及 A、B、C、D 四個選項', 'properties': {'qid': {'description': '題號', 'title': 'Qid', 'type': 'integer'}, 'stem': {'description': '題幹', 'title': 'Stem', 'type': 'string'}, 'A': {'description': '本題的A選項', 'title': 'A', 'type': 'string'}, 'B': {'description': '本題的B選項', 'title': 'B', 'type': 'string'}, 'C': {'description': '本題的C選項', 'title': 'C', 'type': 'string'}, 'D': {'description': '本題的D選項', 'title': 'D', 'type': 'string'}, 'ans': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': '答案', 'title': 'Ans'}}, 'required': ['qid', 'stem', 'A', 'B', 'C', 'D'], 'title': 'MCQ', 'type': 'object'}\n",
      "從以下文字中擷取一題選擇題 (MCQ)。如果原始文字沒有提供答案，則完全省略答案欄位，且不要嘗試推測答案\n",
      "\n",
      "以下開始:\n",
      "-----\n",
      "{text}\n",
      "-----\n",
      "結果：\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = MCQ.model_json_schema()\n",
    "\n",
    "gemma_prompt = PromptTemplate(\n",
    "    \"這是 MCQ 的 JSON schema:\\n\"\n",
    "    f\"{schema}\\n\"\n",
    "    \"從以下文字中擷取一題選擇題 (MCQ)。如果原始文字沒有提供答案，則完全省略答案欄位，且不要嘗試推測答案\\n\\n以下開始:\\n\"\n",
    "    \"-----\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"-----\\n\"\n",
    "    \"結果：\\n\"\n",
    ")\n",
    "\n",
    "print(gemma_prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "869e0924-e08f-415b-9f57-b3911a0b3c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"qid\": 1,\n",
      "  \"stem\": \"常見針灸配穴法中，所指的「四關穴」，為下列何穴位之組合？\",\n",
      "  \"A\": \"上星、日月\",\n",
      "  \"B\": \"合谷、太衝\",\n",
      "  \"C\": \"內關、外關\",\n",
      "  \"D\": \"上關、下關\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = gemma.complete(gemma_prompt.format(text=query))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796a0edb-8fb8-44ea-8317-b5d1ec737da5",
   "metadata": {},
   "source": [
    "# 6. Direct prompting with json mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "49df9e99-c287-4516-9058-fe48affd69e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"qid\": 1,\n",
      "  \"stem\": \"常見針灸配穴法中，所指的「四關穴」，為下列何穴位之組合？\",\n",
      "  \"A\": \"上星、日月\",\n",
      "  \"B\": \"合谷、太衝\",\n",
      "  \"C\": \"內關、外關\",\n",
      "  \"D\": \"上關、下關\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# this show that json_object didn't work in ollama model (but should work in openai model)\n",
    "response = gemma.complete(gemma_prompt.format(text=query), additional_kwargs={'response_format': {\"type\": \"json_object\"}})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d985a1d-756e-4489-8b82-30adb33ebe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use ollama model: gemma3:12b\n"
     ]
    }
   ],
   "source": [
    "json_gemma = get_llm('gemma', json_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02dbc13e-0c2d-44de-b569-ed2c9778fa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"qid\": 1,\n",
      "  \"stem\": \"常見針灸配穴法中，所指的「四關穴」，為下列何穴位之組合？\",\n",
      "  \"A\": \"上星、日月\",\n",
      "  \"B\": \"合谷、太衝\",\n",
      "  \"C\": \"內關、外關\",\n",
      "  \"D\": \"上關、下關\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = json_gemma.complete(gemma_prompt.format(text=query))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "489d4741-0c1e-40c7-bd75-d8e5401c047a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gemma3:12b',\n",
       " 'created_at': '2025-10-02T17:10:49.07529124Z',\n",
       " 'done': True,\n",
       " 'done_reason': 'stop',\n",
       " 'total_duration': 16989334006,\n",
       " 'load_duration': 6316815814,\n",
       " 'prompt_eval_count': 376,\n",
       " 'prompt_eval_duration': 2615835633,\n",
       " 'eval_count': 90,\n",
       " 'eval_duration': 7955849202,\n",
       " 'message': Message(role='assistant', content='{\\n  \"qid\": 1,\\n  \"stem\": \"常見針灸配穴法中，所指的「四關穴」，為下列何穴位之組合？\",\\n  \"A\": \"上星、日月\",\\n  \"B\": \"合谷、太衝\",\\n  \"C\": \"內關、外關\",\\n  \"D\": \"上關、下關\"\\n}', thinking=None, images=None, tool_name=None, tool_calls=None),\n",
       " 'usage': {'prompt_tokens': 376, 'completion_tokens': 90, 'total_tokens': 466}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cf1a85b-e1c0-493e-b2a7-844009cc761a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"qid\": 1,\\n  \"stem\": \"常見針灸配穴法中，所指的「四關穴」，為下列何穴位之組合？\",\\n  \"A\": \"上星、日月\",\\n  \"B\": \"合谷、太衝\",\\n  \"C\": \"內關、外關\",\\n  \"D\": \"上關、下關\"\\n}'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "903eb70b-13bc-4f79-a091-38a40438cd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "Ollama(\n",
       "    model: str,\n",
       "    base_url: str = \u001b[33m'http://localhost:11434'\u001b[39m,\n",
       "    temperature: Optional[float] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    context_window: int = -\u001b[32m1\u001b[39m,\n",
       "    request_timeout: Optional[float] = \u001b[32m30.0\u001b[39m,\n",
       "    prompt_key: str = \u001b[33m'prompt'\u001b[39m,\n",
       "    json_mode: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    additional_kwargs: Optional[Dict[str, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    client: Optional[ollama._client.Client] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    async_client: Optional[ollama._client.AsyncClient] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    is_function_calling_model: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    keep_alive: Union[float, str, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    thinking: Optional[bool] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    *,\n",
       "    callback_manager: llama_index.core.callbacks.base.CallbackManager = <factory>,\n",
       "    system_prompt: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    messages_to_prompt: Annotated[Optional[llama_index.core.llms.llm.MessagesToPromptType], WithJsonSchema(json_schema={\u001b[33m'type'\u001b[39m: \u001b[33m'string'\u001b[39m}, mode=\u001b[38;5;28;01mNone\u001b[39;00m)] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    completion_to_prompt: Annotated[Optional[llama_index.core.llms.llm.CompletionToPromptType], WithJsonSchema(json_schema={\u001b[33m'type'\u001b[39m: \u001b[33m'string'\u001b[39m}, mode=\u001b[38;5;28;01mNone\u001b[39;00m)] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    output_parser: Optional[llama_index.core.types.BaseOutputParser] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    pydantic_program_mode: llama_index.core.types.PydanticProgramMode = <PydanticProgramMode.DEFAULT: \u001b[33m'default'\u001b[39m>,\n",
       "    query_wrapper_prompt: Optional[llama_index.core.prompts.base.BasePromptTemplate] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "Ollama LLM.\n",
       "\n",
       "Visit https://ollama.com/ to download and install Ollama.\n",
       "\n",
       "Run `ollama serve` to start a server.\n",
       "\n",
       "Run `ollama pull <name>` to download a model to run.\n",
       "\n",
       "Examples:\n",
       "    `pip install llama-index-llms-ollama`\n",
       "\n",
       "    ```python\n",
       "    from llama_index.llms.ollama import Ollama\n",
       "\n",
       "    llm = Ollama(model=\"llama2\", request_timeout=60.0)\n",
       "\n",
       "    response = llm.complete(\"What is the capital of France?\")\n",
       "    print(response)\n",
       "    ```\n",
       "\u001b[31mInit docstring:\u001b[39m\n",
       "Create a new model by parsing and validating input data from keyword arguments.\n",
       "\n",
       "Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
       "validated to form a valid model.\n",
       "\n",
       "`self` is explicitly positional-only to allow `self` as a field name.\n",
       "\u001b[31mFile:\u001b[39m           ~/miniconda3/envs/rag30/lib/python3.12/site-packages/llama_index/llms/ollama/base.py\n",
       "\u001b[31mType:\u001b[39m           ModelMetaclass\n",
       "\u001b[31mSubclasses:\u001b[39m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ollama?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1a60007a-1bd3-4d3a-add0-751d18e837f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "OpenAI(\n",
       "    model: str = \u001b[33m'gpt-3.5-turbo'\u001b[39m,\n",
       "    temperature: float = \u001b[32m0.1\u001b[39m,\n",
       "    max_tokens: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    additional_kwargs: Optional[Dict[str, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    max_retries: int = \u001b[32m3\u001b[39m,\n",
       "    timeout: float = \u001b[32m60.0\u001b[39m,\n",
       "    reuse_client: bool = \u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    api_key: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    api_base: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    api_version: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    callback_manager: Optional[llama_index.core.callbacks.base.CallbackManager] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    default_headers: Optional[Dict[str, str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    http_client: Optional[httpx.Client] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    async_http_client: Optional[httpx.AsyncClient] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    openai_client: Optional[openai.OpenAI] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    async_openai_client: Optional[openai.AsyncOpenAI] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    system_prompt: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    messages_to_prompt: Optional[Callable[[Sequence[llama_index.core.base.llms.types.ChatMessage]], str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    completion_to_prompt: Optional[Callable[[str], str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    pydantic_program_mode: llama_index.core.types.PydanticProgramMode = <PydanticProgramMode.DEFAULT: \u001b[33m'default'\u001b[39m>,\n",
       "    output_parser: Optional[llama_index.core.types.BaseOutputParser] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    strict: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    reasoning_effort: Optional[Literal[\u001b[33m'low'\u001b[39m, \u001b[33m'medium'\u001b[39m, \u001b[33m'high'\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    modalities: Optional[List[str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    audio_config: Optional[Dict[str, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    *,\n",
       "    query_wrapper_prompt: Optional[llama_index.core.prompts.base.BasePromptTemplate] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    logprobs: Optional[bool] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    top_logprobs: Annotated[int, Ge(ge=\u001b[32m0\u001b[39m), Le(le=\u001b[32m20\u001b[39m)] = \u001b[32m0\u001b[39m,\n",
       ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "OpenAI LLM.\n",
       "\n",
       "Args:\n",
       "    model: name of the OpenAI model to use.\n",
       "    temperature: a float from 0 to 1 controlling randomness in generation; higher will lead to more creative, less deterministic responses.\n",
       "    max_tokens: the maximum number of tokens to generate.\n",
       "    additional_kwargs: Add additional parameters to OpenAI request body.\n",
       "    max_retries: How many times to retry the API call if it fails.\n",
       "    timeout: How long to wait, in seconds, for an API call before failing.\n",
       "    reuse_client: Reuse the OpenAI client between requests. When doing anything with large volumes of async API calls, setting this to false can improve stability.\n",
       "    api_key: Your OpenAI api key\n",
       "    api_base: The base URL of the API to call\n",
       "    api_version: the version of the API to call\n",
       "    callback_manager: the callback manager is used for observability.\n",
       "    default_headers: override the default headers for API requests.\n",
       "    http_client: pass in your own httpx.Client instance.\n",
       "    async_http_client: pass in your own httpx.AsyncClient instance.\n",
       "\n",
       "Examples:\n",
       "    `pip install llama-index-llms-openai`\n",
       "\n",
       "    ```python\n",
       "    import os\n",
       "    import openai\n",
       "\n",
       "    os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
       "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
       "\n",
       "    from llama_index.llms.openai import OpenAI\n",
       "\n",
       "    llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
       "\n",
       "    stream = llm.stream_complete(\"Hi, write a short story\")\n",
       "\n",
       "    for r in stream:\n",
       "        print(r.delta, end=\"\")\n",
       "    ```\n",
       "\u001b[31mInit docstring:\u001b[39m\n",
       "Create a new model by parsing and validating input data from keyword arguments.\n",
       "\n",
       "Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
       "validated to form a valid model.\n",
       "\n",
       "`self` is explicitly positional-only to allow `self` as a field name.\n",
       "\u001b[31mFile:\u001b[39m           ~/miniconda3/envs/rag30/lib/python3.12/site-packages/llama_index/llms/openai/base.py\n",
       "\u001b[31mType:\u001b[39m           ModelMetaclass\n",
       "\u001b[31mSubclasses:\u001b[39m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OpenAI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d2cd82-74e5-4538-9c9c-667c63956191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag30",
   "language": "python",
   "name": "rag30"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
